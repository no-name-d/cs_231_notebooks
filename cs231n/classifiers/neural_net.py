from __future__ import print_function

from builtins import range
from builtins import object
import numpy as np
import matplotlib.pyplot as plt
from past.builtins import xrange

class TwoLayerNet(object):
    """
    Двухслойная полносвязная нейросеть. Входные данные имеют размерность N. 
    Сеть имеет скрытый слой размером H и выполняет классификацию для C классов.
    Мы обучаем её с функцией потерь softmax и регуляризацией L2. 
    Нейросеть использует нелинейную функцию активации ReLU после первого 
    полносвязного слоя.

    Архитектура:

    входные данные - полносвязный слой - ReLU - полносвязный слой - softmax

    Выходные данные - оценки для каждого класса.
    """

    def __init__(self, input_size, hidden_size, output_size, std=1e-4):
        """
        Инициализация модели. Веса - небольшие рандомные значения, смещения
        равны нулю. Веса и смещения сохраняются в переменной self.params - 
        это словарь со следующими ключами:

        W1: Веса первого слоя размером (D, H)
        b1: Смещения первого слоя размером (H,)
        W2: Веса второго слоя размером (H, C)
        b2: Смещения второго слоя размером (C,)

        Inputs:
        - input_size: Размер входных данных D.
        - hidden_size: Число нейронов H в скрытом слое.
        - output_size: Число классов C.
        """
        self.params = {}
        self.params['W1'] = std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    def loss(self, X, y=None, reg=0.0):
        """
        Вычисление потерь и градиента для двухслойной нейросети.

        Inputs:
        - X: Исходные данные размером (N, D). Каждый элемент X[i] это обучающий образец.
        - y: Вектор обучающих меток. y[i] - это метка для X[i], и каждый y[i] пердставляет
             целое число в диапазоне 0 <= y[i] < C. Этот параметр опциональный; без него
             функция просто вернёт оценки. Если его указать, то она вернёт потери и градиент.
        - reg: Сила регуляризации.

        Returns:
        Если y = None, вовзращает матрицу оценок размером (N, C), где [i, c] - оцеки
        класса c, вычисленные для входа X[i].

        Если y не равен None, возвращает кортеж:
        - loss: Потери (данных и регуляризации) для данного пакета обучающих образцов.
        - grads: Словарь, отображающий названия параметров на их градиенты по отношению
                 к функции потерь; имеет те же ключи, что и self.params.
        """
        # Распаковываем переменные из словаря с параметрами
        W1, b1 = self.params['W1'], self.params['b1']
        W2, b2 = self.params['W2'], self.params['b2']
        N, D = X.shape

        # Выполняем прямой проход
        scores = None
        #############################################################################
        # TODO: Выполнить прямой проход, вычисляя оценки классов для исходных       #
        # данных. Сохраните результат в переменную scores, которая должна быть      #
        # массивом размером (N, C).                                                 #
        #############################################################################
        # *****START OF YOUR CODE*****

        pass

        # *****END OF YOUR CODE*****

        # Если обучающие метки не заданы, на этом всё - возвращаем оценки
        if y is None:
            return scores

        # Вычисляем потери
        loss = None
        #############################################################################
        # TODO: Завершите прямой проход и посчитайте потери. Они должны включать    #
        # как потери данных, так и потери L2-регуляризации для W1 и W2. Сохарните   #
        # результат в переменную loss, которая должна быть скаляром. Используйте    #
        # потери Softmax-классификатора.                                            #
        #############################################################################
        # *****START OF YOUR CODE*****

        pass

        # *****END OF YOUR CODE*****

        # Обратный проход: вычисление градиентов
        grads = {}
        #############################################################################
        # TODO: Выполните обратный проход, вычисляя производные весов и смещений.   #
        # Сохраните результаты в словарь grads. Например, grads['W1'] должен        #
        # содержать градиент для W1 и быть матрицей того же размера.                #
        #############################################################################
        # *****START OF YOUR CODE*****

        pass

        # *****END OF YOUR CODE*****

        return loss, grads

    def train(self, X, y, X_val, y_val,
              learning_rate=1e-3, learning_rate_decay=0.95,
              reg=5e-6, num_iters=100,
              batch_size=200, verbose=False):
        """
        Обучение нейросети с помощью стохастического градиентного спуска.

        Inputs:
        - X: Массив обучающих данных размером (N, D).
        - y: Массив обучающих меток размером (N,); y[i] = c означает, что
             X[i] имеет метку c, где 0 <= c < C.
        - X_val: Массив оценочных данных размером (N_val, D).
        - y_val: Массив оценочных меток размером (N_val,).
        - learning_rate: Скорость обучения для оптимизации (скаляр).
        - learning_rate_decay: Скалярный коэффициент для снижения скорости обучения
                               после каждой эпохи.
        - reg: Сила регуляризации.
        - num_iters: Чишло шагов оптимизации.
        - batch_size: Число обучающих примеров для каждого шага.
        - verbose: boolean; Если true, выводит прогресс во время оптимизации.
        """
        num_train = X.shape[0]
        iterations_per_epoch = max(num_train / batch_size, 1)

        # Используем SGD, чтобы оптимизировать параметры в self.model
        loss_history = []
        train_acc_history = []
        val_acc_history = []

        for it in range(num_iters):
            X_batch = None
            y_batch = None

            #########################################################################
            # TODO: Создайте минипакет со случайными обучающими данными и метками,  #
            # сохраните их в X_batch и y_batch соответственно.                    #
            #########################################################################
            # *****START OF YOUR CODE*****

            pass

            # *****END OF YOUR CODE*****

            # Вычислите потери и градиенты для текущего минипакета
            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)
            loss_history.append(loss)

            #########################################################################
            # TODO: Используйте градиенты из словаря grads для обновления           #
            # параметров нейросети (находятся в словаре self.params) с помощью      #
            # стохастического градиентного спуска.                                  #
            #########################################################################
            # *****START OF YOUR CODE*****

            pass

            # *****END OF YOUR CODE*****

            if verbose and it % 100 == 0:
                print('Итерация %d из %d: потери - %f' % (it, num_iters, loss))

            # Каждую эпоху порверяем обучающую и оценочную точность и понижаем скорость обучения.
            if it % iterations_per_epoch == 0:
                # Проверяем точность
                train_acc = (self.predict(X_batch) == y_batch).mean()
                val_acc = (self.predict(X_val) == y_val).mean()
                train_acc_history.append(train_acc)
                val_acc_history.append(val_acc)

                # Понижаем скорость обучения
                learning_rate *= learning_rate_decay

        return {
          'loss_history': loss_history,
          'train_acc_history': train_acc_history,
          'val_acc_history': val_acc_history,
        }

    def predict(self, X):
        """
        Использование полученных весов для прогнозирования меток.
        Для каждого элемента данных мы прогнозируем метки по каждому из C
        классов, а затем помечаем этот элемент как принадлежащий классу с самой
        высокой оценкой.

        Inputs:
        - X: Массив исходных данных размером (N, D).

        Returns:
        - y_pred: Массив прогнозируемых меток размером (N,). Для всех i 
                  y_pred[i] = c означает, что X[i] принадлежит классу c, где 0 <= c < C.
        """
        y_pred = None

        ###########################################################################
        # TODO: Реализуйте эту функцию, она должна быть ОЧЕНЬ простой!            #
        ###########################################################################
        # *****START OF YOUR CODE*****

        pass

        # *****END OF YOUR CODE*****

        return y_pred
