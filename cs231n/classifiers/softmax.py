from builtins import range
import numpy as np
from random import shuffle
from past.builtins import xrange

def softmax_loss_naive(W, X, y, reg):
    """
    Функция потерь Softmax, наивная реализация (с циклами)

    Исходные данные имеют размерность D, всего есть C классов, и мы оперируем минипакетами
    с N примерами.

    Inputs:
    - W: Массив весов размером (D, C).
    - X: Массив с минипакетом данных размером (N, D).
    - y: Массив обучающих меток размером (N,); y[i] = c означает, что
         X[i] имеет метку c, где 0 <= c < C.
    - reg: (float) сила регуляризации

    Returns: Кортеж, содрежащий:
    - потери (число)
    - градиент по отношению к self.W (массив той же размерности, что и W)
    """
    # Инициализируем потери и градиент нулями.
    loss = 0.0
    dW = np.zeros_like(W)

    #############################################################################
    # TODO: Вычислите потери и градиент softmax с помощью вложенных циклов.     #
    # Сохраните потери в переменную loss, а градиент в переменную dW.           #
    # Не забудьте про регуляризацию!                                            #
    #############################################################################
    # *****START OF YOUR CODE*****

    pass

    # *****END OF YOUR CODE*****

    return loss, dW


def softmax_loss_vectorized(W, X, y, reg):
    """
    Функция потерь SVM, векторизованная реализация.
    """
    # Инициализируем потери и градиент нулями.
    loss = 0.0
    dW = np.zeros_like(W)

    #############################################################################
    # TODO: Вычислите потери и градиент softmax без использования циклов.       #
    # Сохраните потери в переменную loss, а градиент в переменную dW.           #
    # Не забудьте про регуляризацию!                                            #
    #############################################################################
    # *****START OF YOUR CODE*****

    pass

    # *****END OF YOUR CODE*****

    return loss, dW
